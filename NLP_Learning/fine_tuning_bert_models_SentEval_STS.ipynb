{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"gNFewUrtUaFg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716499536301,"user_tz":240,"elapsed":72869,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}},"outputId":"a4880d70-04ad-4bbf-946d-14fd1d708660"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (7.7.1)\n","Collecting ipywidgets\n","  Downloading ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.0)\n","Collecting transformers\n","  Downloading transformers-4.41.1-py3-none-any.whl (9.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting accelerate\n","  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Collecting comm>=0.1.3 (from ipywidgets)\n","  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n","Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\n","Collecting widgetsnbextension~=4.0.10 (from ipywidgets)\n","  Downloading widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jupyterlab-widgets~=3.0.10 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.10)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (67.7.2)\n","Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets)\n","  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.13)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: xxhash, widgetsnbextension, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jedi, dill, comm, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, ipywidgets, transformers, datasets, accelerate\n","  Attempting uninstall: widgetsnbextension\n","    Found existing installation: widgetsnbextension 3.6.6\n","    Uninstalling widgetsnbextension-3.6.6:\n","      Successfully uninstalled widgetsnbextension-3.6.6\n","  Attempting uninstall: ipywidgets\n","    Found existing installation: ipywidgets 7.7.1\n","    Uninstalling ipywidgets-7.7.1:\n","      Successfully uninstalled ipywidgets-7.7.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.41.0\n","    Uninstalling transformers-4.41.0:\n","      Successfully uninstalled transformers-4.41.0\n","Successfully installed accelerate-0.30.1 comm-0.2.2 datasets-2.19.1 dill-0.3.8 ipywidgets-8.1.2 jedi-0.19.1 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 transformers-4.41.1 widgetsnbextension-4.0.10 xxhash-3.4.1\n"]}],"source":["! pip install datasets ipywidgets transformers accelerate -U"]},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"FWJYPqj-JTYq"}},{"cell_type":"code","execution_count":24,"metadata":{"id":"e5OB8685hWHW","executionInfo":{"status":"ok","timestamp":1716503678610,"user_tz":240,"elapsed":122,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}}},"outputs":[],"source":["from typing import Dict, List, Optional, Union, Any, Tuple\n","from datasets import load_dataset, concatenate_datasets, Dataset, DatasetDict\n","from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments, AutoModelForCausalLM, AutoModelForSequenceClassification\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import re\n","from scipy.stats import spearmanr\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import gzip\n","import csv\n","import pandas as pd\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.utils import shuffle\n","from sklearn.metrics import classification_report\n","from sklearn.svm import SVC"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bYyZ-fo-UaFi","outputId":"3b2e054c-7aae-4891-d2ee-736b64d8bf96","executionInfo":{"status":"ok","timestamp":1716500498022,"user_tz":240,"elapsed":3,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["torch.cuda.is_available()"]},{"cell_type":"markdown","source":["## Tokenizer"],"metadata":{"id":"oKd63ogoJZYp"}},{"cell_type":"markdown","metadata":{"id":"xzXAVVK4UaFi"},"source":["The Tokenizer method, for now the maximum length is fixed to 512..."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Wlya304lhWHX","executionInfo":{"status":"ok","timestamp":1716500498022,"user_tz":240,"elapsed":2,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}}},"outputs":[],"source":["class CustomDataTokenizer:\n","    def __init__(self, tokenizer, is_classification=True, max_length = 512):\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.is_classification = is_classification\n","\n","    def __call__(self, data: Dict) -> Dict:\n","        # For Classification Task: Only Text1 is required...\n","        text_columns = ['text1']\n","        if not self.is_classification:\n","            text_columns.append('text2')\n","\n","        tokens_list = []\n","        for text_column in text_columns:\n","            # Tokenization happens here to get in the form which is accepted in the Objective Function...\n","            tokens_list.append(self.tokenizer(data[text_column], max_length=self.max_length, truncation=True))\n","\n","        token = {}\n","        seperate_ids = []\n","        for i, t in enumerate(tokens_list):\n","            for key, val in t.items():\n","                if i == 0:\n","                    token[key] = val\n","                else:\n","                    token[key] += val\n","                if key == 'input_ids':\n","                    seperate_ids += [i] * len(val)\n","\n","        token['labels'] = [int(data['label']) if 'label' in data else -1]\n","        token['seperate_ids'] = seperate_ids\n","\n","        return token"]},{"cell_type":"markdown","source":["## Losses"],"metadata":{"id":"Wf43nyBfJpUi"}},{"cell_type":"markdown","metadata":{"id":"OkSrRaOJUaFj"},"source":["The loss functions"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ZOw5jWY2hWHX","executionInfo":{"status":"ok","timestamp":1716500498022,"user_tz":240,"elapsed":2,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}}},"outputs":[],"source":["def categorical_crossentropy(y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n","    return -(F.log_softmax(y_pred, dim=1) * y_true).sum(dim=1)\n","\n","def cosine_loss(y_true: torch.Tensor, y_pred: torch.Tensor, tau: float = 20.0) -> torch.Tensor:\n","    y_true = y_true[::2, 0]\n","    y_true = (y_true[:, None] < y_true[None, :]).float()\n","    y_pred = F.normalize(y_pred, p=2, dim=1)\n","    y_pred = torch.sum(y_pred[::2] * y_pred[1::2], dim=1) * tau\n","    y_pred = y_pred[:, None] - y_pred[None, :]\n","    y_pred = (y_pred - (1 - y_true) * 1e12).view(-1)\n","    zero = torch.Tensor([0]).to(y_pred.device)\n","    y_pred = torch.concat((zero, y_pred), dim=0)\n","    return torch.logsumexp(y_pred, dim=0)\n","\n","def angle_loss(y_true: torch.Tensor, y_pred: torch.Tensor, tau: float = 1.0):\n","    y_true = y_true[::2, 0]\n","    y_true = (y_true[:, None] < y_true[None, :]).float()\n","\n","    y_pred_re, y_pred_im = torch.chunk(y_pred, 2, dim=1)\n","    a = y_pred_re[::2]\n","    b = y_pred_im[::2]\n","    c = y_pred_re[1::2]\n","    d = y_pred_im[1::2]\n","\n","    z = torch.sum(c**2 + d**2, dim=1, keepdim=True)\n","    re = (a * c + b * d) / z\n","    im = (b * c - a * d) / z\n","\n","    dz = torch.sum(a**2 + b**2, dim=1, keepdim=True)**0.5\n","    dw = torch.sum(c**2 + d**2, dim=1, keepdim=True)**0.5\n","    re /= (dz / dw)\n","    im /= (dz / dw)\n","\n","    y_pred = torch.concat((re, im), dim=1)\n","    y_pred = torch.abs(torch.sum(y_pred, dim=1)) * tau\n","    y_pred = y_pred[:, None] - y_pred[None, :]\n","    y_pred = (y_pred - (1 - y_true) * 1e12).view(-1)\n","    zero = torch.Tensor([0]).to(y_pred.device)\n","    y_pred = torch.concat((zero, y_pred), dim=0)\n","    return torch.logsumexp(y_pred, dim=0)\n","\n","def in_batch_negative_loss(y_true: torch.Tensor,\n","                           y_pred: torch.Tensor,\n","                           tau: float = 20.0,\n","                           negative_weights: float = 0.0) -> torch.Tensor:\n","    device = y_true.device\n","\n","    def make_target_matrix(y_true: torch.Tensor):\n","        idxs = torch.arange(0, y_pred.shape[0]).int().to(device)\n","        y_true = y_true.int()\n","        idxs_1 = idxs[None, :]\n","        idxs_2 = (idxs + 1 - idxs % 2 * 2)[:, None]\n","\n","        idxs_1 *= y_true.T\n","        idxs_1 += (y_true.T == 0).int() * -2\n","\n","        idxs_2 *= y_true\n","        idxs_2 += (y_true == 0).int() * -1\n","\n","        y_true = (idxs_1 == idxs_2).float()\n","        return y_true\n","\n","    neg_mask = make_target_matrix(y_true == 0)\n","\n","    y_true = make_target_matrix(y_true)\n","\n","    y_pred = F.normalize(y_pred, dim=1, p=2)\n","    similarities = y_pred @ y_pred.T\n","    similarities = similarities - torch.eye(y_pred.shape[0]).to(device) * 1e12\n","    similarities = similarities * tau\n","\n","    if negative_weights > 0:\n","        similarities += neg_mask * negative_weights\n","\n","    return categorical_crossentropy(y_true, similarities).mean()"]},{"cell_type":"markdown","metadata":{"id":"I2Xwuv6RUaFk"},"source":["Combining the loss functions with weights"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"adhkDi1bhWHX","executionInfo":{"status":"ok","timestamp":1716500498022,"user_tz":240,"elapsed":2,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}}},"outputs":[],"source":["class TotalLoss:\n","    def __init__(self,\n","                w1: float = 1.0,\n","                w2: float = 1.0,\n","                w3: float = 1.0,\n","                cosine_tau: float = 20.0,\n","                ibn_tau: float = 20.0,\n","                angle_tau: float = 1.0):\n","        self.w1 = w1\n","        self.w2 = w2\n","        self.w3 = w3\n","        self.cosine_tau = cosine_tau\n","        self.ibn_tau = ibn_tau\n","        self.angle_tau = angle_tau\n","\n","    def __call__(self, labels: torch.Tensor, outputs: torch.Tensor) -> torch.Tensor:\n","        loss = 0.\n","        if self.w1 > 0:\n","            loss += self.w1 * cosine_loss(labels, outputs, self.cosine_tau)\n","        if self.w2 > 0:\n","            loss += self.w2 * in_batch_negative_loss(labels, outputs, self.ibn_tau)\n","        if self.w3 > 0:\n","            loss += self.w3 * angle_loss(labels, outputs, self.angle_tau)\n","        return loss"]},{"cell_type":"markdown","source":["## Pooler"],"metadata":{"id":"bXi45wR2KFFv"}},{"cell_type":"markdown","metadata":{"id":"zz8a3YGkUaFk"},"source":["The different Pooling methods, using CLS for now, and Padding Strategy 'Left' for now"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"xyFWXUQXhWHY","executionInfo":{"status":"ok","timestamp":1716500498168,"user_tz":240,"elapsed":148,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}}},"outputs":[],"source":["class Pooler:\n","    def __init__(self,\n","                model,\n","                # ['cls', 'cls_avg', 'last', 'avg', 'max', 'all', 'specific token index']\n","                pooling_strategy: Optional[Union[int, str]] = 'cls',\n","                padding_strategy: Optional[str] = 'left'):\n","        self.model = model\n","        self.pooling_strategy = pooling_strategy\n","        self.padding_strategy = padding_strategy\n","\n","    def __call__(self, inputs) -> Any:\n","        if self.pooling_strategy == 'last':\n","            batch_size = inputs['input_ids'].shape[0]\n","            if self.padding_strategy == 'left':\n","                sequence_lengths = -1\n","            else:\n","                sequence_lengths = inputs[\"attention_mask\"].sum(dim=1) - 1\n","\n","        outputs = self.model(**inputs).last_hidden_state\n","        if self.pooling_strategy == 'cls':\n","            outputs = outputs[:, 0]\n","        elif self.pooling_strategy == 'cls_avg':\n","            outputs = (outputs[:, 0] + torch.mean(outputs, dim=1)) / 2.0\n","        elif self.pooling_strategy == 'last':\n","            outputs = outputs[torch.arange(batch_size, device=outputs.device), sequence_lengths]\n","        elif self.pooling_strategy == 'avg':\n","            outputs = torch.sum(\n","                outputs * inputs[\"attention_mask\"][:, :, None], dim=1) / torch.sum(inputs[\"attention_mask\"])\n","        elif self.pooling_strategy == 'max':\n","            outputs, _ = torch.max(outputs * inputs[\"attention_mask\"][:, :, None], dim=1)\n","        elif self.pooling_strategy == 'all':\n","            return outputs\n","        elif isinstance(self.pooling_strategy, int) or self.pooling_strategy.isnumeric():\n","            return outputs[:, int(self.pooling_strategy)]\n","        return outputs"]},{"cell_type":"markdown","source":["## Trainer"],"metadata":{"id":"yVDqc2HoKXpU"}},{"cell_type":"markdown","metadata":{"id":"KA8qbPXoUaFl"},"source":["The custom trainer method which extends the Trainer method of Transformers"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"0RpRE7z8hWHY","executionInfo":{"status":"ok","timestamp":1716500498168,"user_tz":240,"elapsed":2,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}}},"outputs":[],"source":["class CustomTrainer(Trainer):\n","    def __init__(self, pooler: Pooler, loss_kwargs: Optional[Dict] = None, **kwargs):\n","        super().__init__(**kwargs)\n","        self.pooler = pooler\n","        if loss_kwargs is None:\n","            loss_kwargs = {}\n","        self.loss_fct = TotalLoss(**loss_kwargs)\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.pop(\"labels\", None)\n","        outputs = self.pooler(inputs)\n","        loss = self.loss_fct(labels, outputs)\n","        return (loss, outputs) if return_outputs else loss"]},{"cell_type":"markdown","source":["## Data Collator"],"metadata":{"id":"yDR2QBJiKnBf"}},{"cell_type":"markdown","metadata":{"id":"Bx8iSK42UaFl"},"source":["The custom data collator which works with the trainer"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"xar9ji1NhWHY","executionInfo":{"status":"ok","timestamp":1716500498168,"user_tz":240,"elapsed":2,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}}},"outputs":[],"source":["class CustomDataCollator:\n","    tokenizer = None\n","    padding = 'longest'\n","    max_length: Optional[int] = 512\n","    return_tensors: str = \"pt\"\n","\n","    def __init__(self, tokenizer_base):\n","        self.tokenizer = tokenizer_base\n","\n","    def __call__(self, features: List[Dict], return_tensors: str = \"pt\") -> Dict[str, torch.Tensor]:\n","        if return_tensors is None:\n","            return_tensors = self.return_tensors\n","        has_token_type_ids = \"token_type_ids\" in features[0]\n","\n","        new_features = []\n","        for feature in features:\n","            seperate_ids = feature['seperate_ids']\n","            input_ids = feature['input_ids']\n","            attention_mask = feature['attention_mask']\n","            assert len(seperate_ids) == len(input_ids) == len(attention_mask)\n","\n","            has_token_type_ids = False\n","            if \"token_type_ids\" in feature:\n","                has_token_type_ids = True\n","                token_type_ids = feature['token_type_ids']\n","                assert len(token_type_ids) == len(input_ids)\n","\n","            max_seperate_id = max(seperate_ids)\n","            prev_start_idx = 0\n","            for seperate_id in range(1, max_seperate_id + 1):\n","                start_idx = seperate_ids.index(seperate_id)\n","\n","                new_feature = {}\n","                new_feature['input_ids'] = input_ids[prev_start_idx:start_idx]\n","                new_feature['attention_mask'] = attention_mask[prev_start_idx:start_idx]\n","                if has_token_type_ids:\n","                    new_feature['token_type_ids'] = token_type_ids[prev_start_idx:start_idx]\n","                new_feature['labels'] = feature['labels']\n","                new_features.append(new_feature)\n","                prev_start_idx = start_idx\n","\n","            new_feature = {}\n","            new_feature['input_ids'] = input_ids[prev_start_idx:]\n","            new_feature['attention_mask'] = attention_mask[prev_start_idx:]\n","            if has_token_type_ids:\n","                new_feature['token_type_ids'] = token_type_ids[prev_start_idx:]\n","            new_feature['labels'] = feature['labels']\n","            new_features.append(new_feature)\n","\n","        del features\n","        features = self.tokenizer.pad(\n","            {'input_ids': [feature['input_ids'] for feature in new_features]},\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            return_tensors=return_tensors,\n","        )\n","        features['attention_mask'] = self.tokenizer.pad(\n","            {'input_ids': [feature['attention_mask'] for feature in new_features]},\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            return_tensors=return_tensors,\n","        )['input_ids']\n","        if has_token_type_ids:\n","            features['token_type_ids'] = self.tokenizer.pad(\n","                {'input_ids': [feature['token_type_ids'] for feature in new_features]},\n","                padding=self.padding,\n","                max_length=self.max_length,\n","                return_tensors=return_tensors,\n","            )['input_ids']\n","        features['labels'] = torch.Tensor([feature['labels'] for feature in new_features])\n","\n","        return features"]},{"cell_type":"markdown","source":["## Fit"],"metadata":{"id":"kDsjQhxkKq1Y"}},{"cell_type":"markdown","metadata":{"id":"ajV4GQVDUaFm"},"source":["The fit method which starts the training process, for now a lot of arguments have provided with default value..."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"_41sJ58WhWHY","executionInfo":{"status":"ok","timestamp":1716500498540,"user_tz":240,"elapsed":2,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}}},"outputs":[],"source":["def fit(train_ds,\n","        model_base,\n","        tokenizer_base,\n","        batch_size: int = 32,\n","        output_dir: Optional[str] = 'chk/new_c',\n","        epochs: int = 3,\n","        learning_rate: float = 1e-5,\n","        warmup_steps: int = 1000,\n","        logging_steps: int = 10,\n","        eval_steps: Optional[int] = None,\n","        save_steps: int = 100,\n","        save_strategy: str = 'steps',\n","        save_total_limit: int = 10,\n","        gradient_accumulation_steps: int = 1,\n","        fp16: Optional[bool] = None,\n","        argument_kwargs: Optional[Dict] = None,\n","        trainer_kwargs: Optional[Dict] = None,\n","        loss_kwargs: Optional[Dict] = None):\n","\n","    if argument_kwargs is None:\n","        argument_kwargs = {}\n","    if trainer_kwargs is None:\n","        trainer_kwargs = {}\n","    callbacks = None\n","\n","    pooler = Pooler(model_base)\n","\n","    trainer = CustomTrainer(\n","        pooler=pooler,\n","        model=model_base,\n","        train_dataset=train_ds,\n","        loss_kwargs=loss_kwargs,\n","        tokenizer=tokenizer_base,\n","        args=TrainingArguments(\n","            per_device_train_batch_size=batch_size,\n","            gradient_accumulation_steps=gradient_accumulation_steps,\n","            warmup_steps=warmup_steps,\n","            num_train_epochs=epochs,\n","            learning_rate=learning_rate,\n","            fp16=fp16,\n","            logging_steps=logging_steps,\n","            save_strategy=save_strategy,\n","            eval_steps=eval_steps,\n","            save_steps=save_steps,\n","            output_dir=output_dir,\n","            save_total_limit=save_total_limit,\n","            load_best_model_at_end=False,\n","            ddp_find_unused_parameters=None,\n","            label_names=['labels', 'seperate_ids', 'extra'],\n","            **argument_kwargs,\n","        ),\n","        callbacks=callbacks,\n","        data_collator=CustomDataCollator(\n","            tokenizer_base\n","        ),\n","        **trainer_kwargs\n","    )\n","\n","    trainer.train()\n","    return model_base, tokenizer_base, pooler"]},{"cell_type":"markdown","source":["## Embeddings"],"metadata":{"id":"OpiS9-RxKuun"}},{"cell_type":"markdown","metadata":{"id":"CovZGEUAUaFm"},"source":["The encode method to generate embeddings"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"SASA4fFghWHZ","executionInfo":{"status":"ok","timestamp":1716500501695,"user_tz":240,"elapsed":107,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}}},"outputs":[],"source":["def encode(inputs: Union[List[str], Tuple[str], List[Dict], str],\n","            model,\n","            pooler,\n","            tokenizer,\n","            max_length: Optional[int] = 512,\n","            to_numpy: bool = True,\n","            device: Optional[Any] = 'cuda:0'):\n","        if device is None:\n","            device = 'cpu'\n","        model.to(device)\n","        model.eval()\n","\n","        tokens = tokenizer(\n","            inputs,\n","            padding='longest',\n","            max_length=max_length,\n","            truncation=True,\n","            return_tensors='pt')\n","        tokens.to(device)\n","        with torch.no_grad():\n","            output = pooler(tokens)\n","        if to_numpy:\n","            return output.float().detach().cpu().numpy()\n","        return output"]},{"cell_type":"markdown","source":["# Execution"],"metadata":{"id":"npUyhInUK6qZ"}},{"cell_type":"markdown","source":["## Data Import"],"metadata":{"id":"n_yqmnGuKxOH"}},{"cell_type":"markdown","metadata":{"id":"vfBmu_0-6wGZ"},"source":["SentEval Datasets Import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b0x3s3P_6zAe"},"outputs":[],"source":["def get_senteval_binary_data(dataset):\n","    match dataset:\n","        case 'CR':\n","            pos_file = open(\"Data/custrev.pos\", \"r\")\n","            neg_file = open(\"Data/custrev.neg\", \"r\")\n","        case 'MPQA':\n","            pos_file = open(\"Data/mpqa.pos\", \"r\")\n","            neg_file = open(\"Data/mpqa.neg\", \"r\")\n","        case 'MR':\n","            pos_file = open(\"Data/rt-polarity.pos\", \"r\")\n","            neg_file = open(\"Data/rt-polarity.neg\", \"r\")\n","        case 'SUBJ':\n","            pos_file = open(\"Data/subj.objective\", \"r\")\n","            neg_file = open(\"Data/subj.subjective\", \"r\")\n","\n","    df_pos = pd.DataFrame()\n","    pos_content = pos_file.readlines()\n","    pos_file.close()\n","    df_pos['sentence'] = pos_content\n","    labels = np.ones(len(pos_content))\n","    df_pos['label'] = labels.astype('int')\n","\n","    df_neg = pd.DataFrame()\n","    neg_content = neg_file.readlines()\n","    neg_file.close()\n","    df_neg['sentence'] = neg_content\n","    labels = np.zeros(len(neg_content))\n","    df_neg['label'] = labels.astype('int')\n","\n","    df = pd.concat([df_pos, df_neg], axis=0, ignore_index=True)\n","    df['sentence'] = df['sentence'].str.replace('\\n', '')\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kG8RjUks61wY"},"outputs":[],"source":["# SST -> We use the Binary classification...\n","def get_sst_data():\n","    train_file = open(\"Data/sentiment-train\", \"r\")\n","    train = train_file.readlines()\n","    train_file.close()\n","\n","    sentence_train = []\n","    label_train = []\n","    for sentence in train:\n","        sentence = sentence.strip()\n","        label = int(sentence[len(sentence) - 1])\n","        sentence = sentence[:-1].strip()\n","        sentence_train.append(sentence)\n","        label_train.append(label)\n","\n","    df_train = pd.DataFrame({'sentence': sentence_train, 'label': label_train})\n","\n","    test_file = open(\"Data/sentiment-test\", \"r\")\n","    test = test_file.readlines()\n","    test_file.close()\n","\n","    sentence_test = []\n","    label_test = []\n","    for sentence in test:\n","        sentence = sentence.strip()\n","        label = int(sentence[len(sentence) - 1])\n","        sentence = sentence[:-1].strip()\n","        sentence_test.append(sentence)\n","        label_test.append(label)\n","\n","    df_test = pd.DataFrame({'sentence': sentence_test, 'label': label_test})\n","    df = pd.concat([df_train, df_test], axis=0, ignore_index=True)\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Y3rYibz620e"},"outputs":[],"source":["def get_senteval_dataset(name):\n","    match name:\n","        case 'CR':\n","            return get_senteval_binary_data('CR')\n","        case 'MPQA':\n","            return get_senteval_binary_data('MPQA')\n","        case 'MR':\n","            return get_senteval_binary_data('MR')\n","        case 'SST':\n","            return get_sst_data()\n","        case 'SUBJ':\n","            return get_senteval_binary_data('SUBJ')"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"ysILLEOe7CJr","executionInfo":{"status":"ok","timestamp":1716500506120,"user_tz":240,"elapsed":117,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}}},"outputs":[],"source":["# senteval_datasets = ['CR', 'MPQA', 'MR', 'SUBJ']#, 'SST']\n","senteval_datasets = ['DATA']"]},{"cell_type":"code","source":["def get_custom_data():\n","  benign_data = pd.read_csv(\"/content/drive/MyDrive/Thesis_Projects/URL_detection/Benign.csv\")\n","\n","  malicious_data = pd.read_csv(\"/content/drive/MyDrive/Thesis_Projects/URL_detection/Malicious.csv\")\n","\n","  df1 = pd.DataFrame(benign_data)\n","  df2 = pd.DataFrame(malicious_data)\n","\n","  x = df1.sample(25000)\n","  y = df2.sample(25000)\n","\n","  data = pd.concat([x,y], axis=0)\n","  data = shuffle(data)\n","  data.rename(columns={'url': 'sentence'}, inplace=True)\n","  return data"],"metadata":{"id":"OioAZXb0WeFP","executionInfo":{"status":"ok","timestamp":1716500507175,"user_tz":240,"elapsed":110,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DxTeegGtWgzg","executionInfo":{"status":"ok","timestamp":1716500510430,"user_tz":240,"elapsed":735,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}},"outputId":"f575816b-5ef0-4b38-e9a3-c6e82545bba5"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"Qi9izGqz7rC7"},"source":["STS Datasets Import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SNHC4DNl7v7T"},"outputs":[],"source":["def get_sts_dataset(dataset_name):\n","    match dataset_name:\n","        case 'STS-B':\n","            dataset = load_dataset('mteb/stsbenchmark-sts', split='test')\n","        case 'STS12':\n","            dataset = load_dataset('mteb/sts12-sts', split='test')\n","        case 'STS13':\n","            dataset = load_dataset('mteb/sts13-sts', split='test')\n","        case 'STS14':\n","            dataset = load_dataset('mteb/sts14-sts', split='test')\n","        case 'STS15':\n","            dataset = load_dataset('mteb/sts15-sts', split='test')\n","        case 'STS16':\n","            dataset = load_dataset('mteb/sts16-sts', split='test')\n","        case 'SICK-R':\n","            dataset = load_dataset('mteb/sickr-sts', split='test')\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMWL7F6P8EG6"},"outputs":[],"source":["sts_datasets = ['STS-B', 'STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'SICK-R']"]},{"cell_type":"markdown","metadata":{"id":"LlAwf-fRaFIL"},"source":["## Objective Function Combinations"]},{"cell_type":"markdown","source":["Using the 3 provided objective functions and their 7 possible combinations for now"],"metadata":{"id":"lGj_X7A8LAwZ"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"s9vafqNKa_-t","executionInfo":{"status":"ok","timestamp":1716500513563,"user_tz":240,"elapsed":105,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}}},"outputs":[],"source":["def get_objective_function_weights(combi):\n","    match combi:\n","        case 'Cosine': return (1, 0, 0)\n","        case 'In-Batch Negatives': return (0, 1, 0)\n","        case 'Angle': return (0, 0, 1)\n","        case 'Cosine + In-Batch Negatives': return (1, 1, 0)\n","        case 'Cosine + Angle': return (1, 0, 1)\n","        case 'In-Batch Negatives + Angle': return (0, 1, 1)\n","        case 'Cosine + In-Batch Negatives + Angle': return (1, 1, 1)"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"hAEJCLCEaQ0Q","executionInfo":{"status":"ok","timestamp":1716503611715,"user_tz":240,"elapsed":109,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}}},"outputs":[],"source":["objective_functions = [\n","    # 'Cosine',\n","    # 'In-Batch Negatives',\n","    'Angle',\n","    # 'Cosine + In-Batch Negatives',\n","    # 'Cosine + Angle',\n","    # 'In-Batch Negatives + Angle',\n","    # 'Cosine + In-Batch Negatives + Angle'\n","]"]},{"cell_type":"markdown","source":["## Language Models"],"metadata":{"id":"xszL7wkILGFh"}},{"cell_type":"markdown","metadata":{"id":"CGBEfvPVhWHZ"},"source":["Base Model Selection"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"UeMjLiE--JRS","executionInfo":{"status":"ok","timestamp":1716500523295,"user_tz":240,"elapsed":127,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}}},"outputs":[],"source":["models = [\n","    # 'bert-base-uncased',\n","    # 'bert-base-cased',\n","    # 'bert-large-uncased',\n","    # 'bert-large-cased'\n","    # 'FacebookAI/roberta-base',\n","    # 'sentence-transformers/all-mpnet-base-v2',\n","    'princeton-nlp/sup-simcse-roberta-large'\n","]"]},{"cell_type":"markdown","source":["## Driver Functions"],"metadata":{"id":"KciCira7LJpg"}},{"cell_type":"markdown","source":["### SentEval"],"metadata":{"id":"w8VR7dhZLh55"}},{"cell_type":"code","execution_count":25,"metadata":{"id":"FtecQezwb5Ui","executionInfo":{"status":"ok","timestamp":1716503690490,"user_tz":240,"elapsed":177,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}}},"outputs":[],"source":["def driver_senteval():\n","    results_matrix = []\n","\n","    for model in models:\n","        results_obj_matrix = []\n","        for objective in objective_functions:\n","            # Objective Functions Preparation...\n","            w1_combi, w2_combi, w3_combi = get_objective_function_weights(objective)\n","            results_obj_ds_matrix = []\n","\n","            for dataset in senteval_datasets:\n","                # Model Preparation...\n","                tokenizer_base = AutoTokenizer.from_pretrained(model)\n","                model_base = AutoModelForSequenceClassification.from_pretrained(model)\n","\n","                # Dataset Preparation...\n","                # df = get_senteval_dataset(dataset)\n","                df = get_custom_data()\n","                ds = Dataset.from_pandas(df)\n","                ds = ds.rename_column('sentence', 'text1')\n","                if dataset == 'MR':\n","                    ds = concatenate_datasets([ds.select(range(0, 231)), ds.select(range(233, 7463))])\n","\n","                split_ds = ds.train_test_split(test_size=0.3, seed=42)\n","                ds_train = split_ds['train']\n","                ds_test = split_ds['test']\n","\n","                # Tokenization...\n","                train_ds = ds_train.shuffle().map(CustomDataTokenizer(tokenizer_base), num_proc=8)\n","\n","                # Model Training...\n","                model_new, tokenizer_new, pooler_new = fit(\n","                    train_ds=train_ds,\n","                    model_base=model_base,\n","                    tokenizer_base=tokenizer_base,\n","                    output_dir='chk/c',\n","                    batch_size=20,\n","                    epochs=5,\n","                    learning_rate=2e-5,\n","                    save_steps=0,\n","                    eval_steps=100,\n","                    warmup_steps=0,\n","                    gradient_accumulation_steps=1,\n","                    loss_kwargs={\n","                        'w1': w1_combi,\n","                        'w2': w2_combi,\n","                        'w3': w3_combi,\n","                        'cosine_tau': 20,\n","                        'ibn_tau': 20,\n","                        'angle_tau': 1.0\n","                    },\n","                    fp16=True,\n","                    logging_steps=1000\n","                )\n","\n","                # Embedding Generation for Train and Test sets... Doing line-by-line embeddings for now...\n","                emb_train = []\n","                for sentence in ds_train['text1']:\n","                    emb_train.append(encode(sentence, model_new, pooler_new, tokenizer_new)[0])\n","\n","                emb_test = []\n","                for sentence in ds_test['text1']:\n","                    emb_test.append(encode(sentence, model_new, pooler_new, tokenizer_new)[0])\n","\n","                # Conversion into Numpy Array...\n","                emb_train = np.array(emb_train)\n","                emb_test = np.array(emb_test)\n","\n","                # Classification...\n","                # lr = LogisticRegression(max_iter=10000)\n","                lr = SVC()\n","                lr.fit(emb_train, ds_train['label'])\n","                results_obj_ds_matrix.append(lr.score(emb_test, ds_test['label']))\n","            results_obj_matrix.append(results_obj_ds_matrix)\n","        results_matrix.append(results_obj_matrix)\n","    return results_matrix"]},{"cell_type":"markdown","source":["### STS"],"metadata":{"id":"Aoik9A2OLk5R"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kj-Y2cnGhWHc"},"outputs":[],"source":["def calculate_cosine_similarity(sentence1_vec, sentence2_vec):\n","    cosine_similarity = np.dot(sentence1_vec, sentence2_vec) / (np.linalg.norm(sentence1_vec) * np.linalg.norm(sentence2_vec))\n","    return cosine_similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XWhPIBI7AAWo"},"outputs":[],"source":["def calculate_Spearman_rank_correlation_coefficient(scores, scores_actual):\n","    sc, _ = spearmanr(scores, scores_actual)\n","    return sc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dMgwpWqAAWo"},"outputs":[],"source":["def driver_sts():\n","    results_matrix = []\n","\n","    for model in models:\n","        results_obj_matrix = []\n","        for objective in objective_functions:\n","            # Objective Functions Preparation...\n","            w1_combi, w2_combi, w3_combi = get_objective_function_weights(objective)\n","            results_obj_ds_matrix = []\n","\n","            for dataset in sts_datasets:\n","                # Model Preparation...\n","                tokenizer_base = AutoTokenizer.from_pretrained(model)\n","                model_base = AutoModel.from_pretrained(model)\n","\n","                # Dataset Preparation...\n","                ds = get_sts_dataset(dataset)\n","                ds = ds.rename_column('sentence1', 'text1')\n","                ds = ds.rename_column('sentence2', 'text2')\n","                ds = ds.rename_column('score', 'label')\n","\n","                split_ds = ds.train_test_split(test_size=0.3, seed=42)\n","                ds_train = split_ds['train']\n","                ds_test = split_ds['test']\n","\n","                # Tokenization of train dataset...\n","                train_ds = ds_train.shuffle().map(CustomDataTokenizer(tokenizer_base, is_classification=False), num_proc=8)\n","\n","                # Model Training...\n","                model_new, tokenizer_new, pooler_new = fit(\n","                    train_ds=train_ds,\n","                    model_base=model_base,\n","                    tokenizer_base=tokenizer_base,\n","                    output_dir='chk/c',\n","                    batch_size=32,\n","                    epochs=5,\n","                    learning_rate=2e-5,\n","                    save_steps=0,\n","                    eval_steps=100,\n","                    warmup_steps=0,\n","                    gradient_accumulation_steps=1,\n","                    loss_kwargs={\n","                        'w1': w1_combi,\n","                        'w2': w2_combi,\n","                        'w3': w3_combi,\n","                        'cosine_tau': 20,\n","                        'ibn_tau': 20,\n","                        'angle_tau': 1.0\n","                    },\n","                    fp16=True,\n","                    logging_steps=1000\n","                )\n","\n","                # Generating embeddings of STS dataset using the newly trained model...\n","                emb_sentence_1 = encode(ds_test['text1'], model_new, pooler_new, tokenizer_new) # generating embeddings for test set sentence 1\n","                emb_sentence_2 = encode(ds_test['text2'], model_new, pooler_new, tokenizer_new) # generating embeddings for test set sentence 2\n","\n","                # Calculating Spearman for AnglE...\n","                cos_score = []\n","                for i in range(emb_sentence_1.shape[0]):\n","                    cos_score.append(calculate_cosine_similarity(emb_sentence_1[i], emb_sentence_2[i]))\n","\n","                spearman = calculate_Spearman_rank_correlation_coefficient(cos_score, ds_test['label'])\n","                results_obj_ds_matrix.append(spearman)\n","            results_obj_matrix.append(results_obj_ds_matrix)\n","        results_matrix.append(results_obj_matrix)\n","    return results_matrix"]},{"cell_type":"markdown","source":["### Running"],"metadata":{"id":"YOkPXS5_LXM5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LGWYzer1AAWo"},"outputs":[],"source":["# results_matrix_sts = driver_sts()"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"M2T82nNvfW6T","colab":{"base_uri":"https://localhost:8080/","height":393,"referenced_widgets":["2a90ffdd61f340e3bff10fbc902a9f17","4c6c7209922d4614a656d591fd17cd85","d5ae97a3fa7e4c61881071b5c9668bc2","86c8e2ea9bcc450e93536a90b670cca0","2613a6733bd44a07b5327e44e78eca89","56cfbd8896dd4d2e8279228758cff543","1fed3a1eb6014e7498ebc31b95bf3ee4","8937c17f6fbb4e98a7e303cbffd96106","430f64c6ad8b4f00b71fe300a400f5a2","394b13a6aa284dc394dbf69d55865466","66cc3744bc4a4de4a8f2124ee790dfc5"]},"executionInfo":{"status":"error","timestamp":1716503699336,"user_tz":240,"elapsed":5869,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}},"outputId":"12282ff6-6b93-4f7c-bf09-e1ba76b82ccb"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=8):   0%|          | 0/35000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a90ffdd61f340e3bff10fbc902a9f17"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'SequenceClassifierOutput' object has no attribute 'last_hidden_state'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-e834e6c4bb97>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_matrix_senteval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver_senteval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-25-c7ea93ca9717>\u001b[0m in \u001b[0;36mdriver_senteval\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;31m# Model Training...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 model_new, tokenizer_new, pooler_new = fit(\n\u001b[0m\u001b[1;32m     33\u001b[0m                     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mmodel_base\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-b127e51ac92e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(train_ds, model_base, tokenizer_base, batch_size, output_dir, epochs, learning_rate, warmup_steps, logging_steps, eval_steps, save_steps, save_strategy, save_total_limit, gradient_accumulation_steps, fp16, argument_kwargs, trainer_kwargs, loss_kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1885\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1886\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2216\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3237\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3238\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3240\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-1e509667e424>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_outputs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-b644687a2ec0>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0msequence_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooling_strategy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cls'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'SequenceClassifierOutput' object has no attribute 'last_hidden_state'"]}],"source":["results_matrix_senteval = driver_senteval()"]},{"cell_type":"markdown","source":["### Saving the Results"],"metadata":{"id":"6m5rbe2hLZeo"}},{"cell_type":"code","source":["results_matrix_senteval"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6t0CIcckk2wl","executionInfo":{"status":"ok","timestamp":1716503408863,"user_tz":240,"elapsed":105,"user":{"displayName":"Vikrant Mehla","userId":"00160510751617329808"}},"outputId":"26aca0fb-e2f9-4fe3-df93-29f0b401879f"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[[0.49566666666666664]]]"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YdDsIykFAAWt"},"outputs":[],"source":["with open('BERT_SentEval_Results.npy', 'wb') as f:\n","    np.save(f, results_matrix_senteval)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZAJd7esKAAWu"},"outputs":[],"source":["with open('BERT_STS_Results.npy', 'wb') as f:\n","    np.save(f, results_matrix_sts)"]},{"cell_type":"markdown","metadata":{"id":"-W8o9ZmMUaFn"},"source":["# NLI"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYV5wydqUaFn"},"outputs":[],"source":["def load_all_nli(exclude_neutral=True):\n","    label_mapping = {\n","        'entailment': 1,  # '0' (entailment)\n","        'neutral': 1,\n","        'contradiction': 0   # '2' (contradiction)\n","    }\n","    data = []\n","    with gzip.open('AllNLI.tsv.gz', 'rt', encoding='utf8') as fIn:\n","        reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n","        for row in reader:\n","            if row['split'] == 'train' and row['label'] != 'neutral':\n","                if exclude_neutral and row['label'] == 'neutral':\n","                    continue\n","                sent1 = row['sentence1'].strip()\n","                sent2 = row['sentence2'].strip()\n","                data.append({'text1': sent1, 'text2': sent2, 'label': label_mapping[row['label']]})\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dwC1copcUaFn"},"outputs":[],"source":["def preprocess_nli():\n","    train_data = load_all_nli()\n","    nli_dataset = {}\n","    train_ds = Dataset.from_list(train_data)\n","    nli_dataset['train'] = train_ds\n","    nli_dataset = DatasetDict(nli_dataset)\n","    ds_train = nli_dataset['train']\n","    return ds_train"]},{"cell_type":"markdown","metadata":{"id":"E-k7PuBPhWHb"},"source":["Training with AnglE losses..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4q85vBUkUaFn"},"outputs":[],"source":["def train_nli(ds_train, model_base, tokenizer_base):\n","    train_ds = ds_train.shuffle().map(CustomDataTokenizer(tokenizer_base), num_proc=8)\n","    model_new, tokenizer_new, pooler_new = fit(\n","        train_ds=train_ds,\n","        model_base=model_base,\n","        tokenizer_base=tokenizer_base,\n","        output_dir='chk/c',\n","        batch_size=32,\n","        epochs=5,\n","        learning_rate=2e-5,\n","        save_steps=0,\n","        eval_steps=100,\n","        warmup_steps=0,\n","        gradient_accumulation_steps=1,\n","        loss_kwargs={\n","            'w1': 1.0,\n","            'w2': 1.0,\n","            'w3': 1.0\n","            'cosine_tau': 20,\n","            'ibn_tau': 20,\n","            'angle_tau': 1.0\n","        },\n","        fp16=True,\n","        logging_steps=1000\n","    )\n","    return model_new, tokenizer_new, pooler_new"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"collapsed_sections":["FWJYPqj-JTYq","oKd63ogoJZYp","Wf43nyBfJpUi","bXi45wR2KFFv","yVDqc2HoKXpU","yDR2QBJiKnBf","kDsjQhxkKq1Y","OpiS9-RxKuun","n_yqmnGuKxOH","LlAwf-fRaFIL","xszL7wkILGFh","w8VR7dhZLh55","Aoik9A2OLk5R","YOkPXS5_LXM5","6m5rbe2hLZeo","-W8o9ZmMUaFn"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2a90ffdd61f340e3bff10fbc902a9f17":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4c6c7209922d4614a656d591fd17cd85","IPY_MODEL_d5ae97a3fa7e4c61881071b5c9668bc2","IPY_MODEL_86c8e2ea9bcc450e93536a90b670cca0"],"layout":"IPY_MODEL_2613a6733bd44a07b5327e44e78eca89","tabbable":null,"tooltip":null}},"4c6c7209922d4614a656d591fd17cd85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_56cfbd8896dd4d2e8279228758cff543","placeholder":"​","style":"IPY_MODEL_1fed3a1eb6014e7498ebc31b95bf3ee4","tabbable":null,"tooltip":null,"value":"Map (num_proc=8): 100%"}},"d5ae97a3fa7e4c61881071b5c9668bc2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_8937c17f6fbb4e98a7e303cbffd96106","max":35000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_430f64c6ad8b4f00b71fe300a400f5a2","tabbable":null,"tooltip":null,"value":35000}},"86c8e2ea9bcc450e93536a90b670cca0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_394b13a6aa284dc394dbf69d55865466","placeholder":"​","style":"IPY_MODEL_66cc3744bc4a4de4a8f2124ee790dfc5","tabbable":null,"tooltip":null,"value":" 35000/35000 [00:03&lt;00:00, 10899.99 examples/s]"}},"2613a6733bd44a07b5327e44e78eca89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56cfbd8896dd4d2e8279228758cff543":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fed3a1eb6014e7498ebc31b95bf3ee4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"8937c17f6fbb4e98a7e303cbffd96106":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"430f64c6ad8b4f00b71fe300a400f5a2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"394b13a6aa284dc394dbf69d55865466":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66cc3744bc4a4de4a8f2124ee790dfc5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}}}}},"nbformat":4,"nbformat_minor":0}